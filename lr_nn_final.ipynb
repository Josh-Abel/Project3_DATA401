{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Layers**: an Integer value representing the total number of hidden layers in the network (input and output layers are extra).\n",
    " \n",
    " - **Nodes**: an integer array of size [0,..,Layers+1] containing the dimensions of the neural\n",
    "network. Nodes[0] shall represent the input size (typically, 50), Nodes[Layers+1]\n",
    "shall represent the number of output nodes (typically, 1). All other values Nodes[i]\n",
    "represent the number of nodes in hidden layer i.\n",
    "\n",
    " - **NNodes**: a possible alternative to the Nodes parameter for situations where you want\n",
    "each hidden layer of the neural network to be of the same size. In this case, the size of\n",
    "the output layer is assumed to be 1, and the size of the input layer can be inferred from\n",
    "the dataset.\n",
    "\n",
    " - **Activations**: an array of size [0,..,Layers+1] (for the sake of compatibility) in which\n",
    "Activations[0] and Activations[Layers+1] are not used, while all other\n",
    "Activations[i] values are labels indicating the activation function used in layer i.\n",
    "This allows you to build neural networks with different activation functions in each layer.\n",
    "\n",
    " - **ActivationFn**: a possible alternative to Activations when all hidden layers of your neural\n",
    "network use the same activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    #Layers: an Integer value representing the total number of hidden layers in the network \n",
    "    #        (input and output layers are extra)\n",
    "\n",
    "    def __init__(self, Layers, Nodes, NNodes, Activations, ActivationFn, a=None):\n",
    "        self.Layers = Layers\n",
    "        self.Nodes = Nodes\n",
    "        self.NNodes = NNodes\n",
    "        self.Activations = Activations\n",
    "        self.ActivationFn = ActivationFn\n",
    "        self.a = a # The coefficient used if using leaky Relu as the activation function, default is None\n",
    "        self.weights = None\n",
    "        \n",
    "       \n",
    "        \n",
    "    # Forward Pass\n",
    "        \n",
    "    def Relu(self,e):\n",
    "        return max(0,e)\n",
    "    \n",
    "    def leakyRelu(self,e):\n",
    "        if e > 0:\n",
    "            return e\n",
    "        else:\n",
    "            return self.a*e\n",
    "        \n",
    "    def sigmoid(self,e):\n",
    "        return 1/(1+np.exp(1)**-e)\n",
    "    \n",
    "    def tanh(self,e):\n",
    "        return 2*self.sigmoid(2*e) - 1\n",
    "    \n",
    "    def applyActivation(self,layer,i):\n",
    "        acttype = self.Activations[i]\n",
    "        if acttype == \"Relu\":\n",
    "            return layer.applymap(self.Relu)\n",
    "        elif acttype == \"leaky\":\n",
    "            return layer.applymap(self.leakyRelu)\n",
    "        elif acttype == \"sigmoid\":\n",
    "            return layer.applymap(self.sigmoid)\n",
    "        elif acttype == \"tanh\":\n",
    "            return layer.applymap(self.tanh)\n",
    "    \n",
    "    def loss(self,z,y):\n",
    "        # Performs L2 loss (for this project)\n",
    "        L = 0.5*((np.array(z)-np.array(y))**2) # Assumes the squaring is element wise\n",
    "        L = np.sum(L) * (1/len(z)) # Take average of all the losses\n",
    "        return L     \n",
    "        \n",
    "\n",
    "\n",
    "    def forward_pass(self, X, y, weights):\n",
    "            # Assume X already has a column of ones for bias term.\n",
    "            # Assume weights include the weights for the bias term when going into next layer\n",
    "\n",
    "            savings = [X]\n",
    "\n",
    "            # From input layer to first hidden layer\n",
    "            h = X.dot(weights[0]) # Get first hidden layer without the bias node added in\n",
    "            h['ones'] = 1 # Add in bias node to the hidden layer\n",
    "            savings.append(h) # Saving intermediate values\n",
    "            hact = self.applyActivation(h,0) # Perform activation\n",
    "            hact['ones'] = 1\n",
    "            savings.append(hact) # Saving intermediate values\n",
    "            h = hact\n",
    "\n",
    "            for i in range(1,len(weights)):\n",
    "                if i != len(weights)-1: # A hidden layer\n",
    "                    h = h.dot(weights[i])\n",
    "                    h['ones'] = 1 # Add in bias node to the hidden layer\n",
    "                    savings.append(h) # Saving intermediate values\n",
    "                    hact = self.applyActivation(h,i) # Perform activation\n",
    "                    hact['ones'] = 1\n",
    "                    savings.append(hact) # Saving intermediate values\n",
    "                    h = hact\n",
    "                else: # For Z value/vector\n",
    "                    z = h.dot(weights[i])\n",
    "                    savings.append(z)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    L = self.loss(z,y)\n",
    "                    savings.append(L) # Are we saving average loss over the batch?\n",
    "            return savings\n",
    "    \n",
    "    # Backwards pass\n",
    "    def J_loss(self,z,y):\n",
    "        B = len(y)\n",
    "        #print(z.subtract(y,axis=0))\n",
    "        #J = (1/B)*(np.array(z) - np.array(y))\n",
    "        J = (1/B)*(z.subtract(y,axis=0))\n",
    "        return J\n",
    "    \n",
    "    def J_sigma(self, X, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            S = (1/(1+np.exp(-X)))\n",
    "            return S.multiply(1-S)\n",
    "        elif activation == \"tanh\":\n",
    "            return 1-(X**2)\n",
    "        elif activation == \"Relu\":\n",
    "            return (X > 0).astype(int)\n",
    "        elif activation == \"leaky\":\n",
    "            return (X>0).astype(int) +  self.a*(X<0).astype(int)\n",
    "        \n",
    "    def J_inputlayer(self,J,w):\n",
    "        #return J.dot(w.T)\n",
    "        #print(w)\n",
    "        #w = pd.DataFrame(w).drop([len(w)-1],axis=0)\n",
    "        w = pd.DataFrame(w).iloc[:-1]\n",
    "        return J.dot(w.T)\n",
    "    \n",
    "    def J_weight(self,J,X):\n",
    "        return np.array(X.T).dot(J)\n",
    "    \n",
    "    def back_propagation(self,X,y,intermediates,weights,lr,batch):\n",
    "        J = pd.DataFrame(self.J_loss(intermediates[-2],y)) # Compute the jacobian of the loss layer evaluated at z\n",
    "        w_on = True\n",
    "        w_count = len(weights)-1\n",
    "        act_count = len(self.Activations) - 1\n",
    "        for i in range(-3,-len(intermediates),-1):\n",
    "            if w_on:\n",
    "                J_wn = self.J_weight(J,intermediates[i])  # Calculate the jacobian of the weights evaluated at sigma\n",
    "                J = self.J_inputlayer(J,weights[w_count])  # Update jacobian by computing the jacobian of dense layer wrt input\n",
    "                weights[w_count] = weights[w_count] - lr*J_wn*(1/batch) # Update the weights\n",
    "                w_count = w_count - 1 # Update the index for the next set of weights\n",
    "                w_on = False # Next derivative evaluated at intermediates[i] will not update the weights\n",
    "            else:\n",
    "                J = np.multiply(J,self.J_sigma(intermediates[i].drop(\"ones\",axis=1), self.Activations[act_count])) # For activation, we use element-wise multiplication\n",
    "                w_on = True\n",
    "                act_count = act_count-1\n",
    "        # Update last set of weights (W_1)\n",
    "        J_w1 = self.J_weight(J,intermediates[-len(intermediates)])\n",
    "        weights[w_count] = weights[w_count] - lr*J_w1*(1/batch)\n",
    "        return weights\n",
    "\n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    def setup(self,width):\n",
    "        if self.NNodes != None:\n",
    "            self.Nodes = [width] + ([self.NNodes]*self.Layers) + [1]\n",
    "        if self.ActivationFn != None:\n",
    "            self.Activations = [self.ActivationFn]*(self.Layers+1) \n",
    "            \n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "        for i in range(len(self.Nodes)-1):\n",
    "            M = self.Nodes[i] \n",
    "            N = self.Nodes[i+1] \n",
    "            if self.Activations[i] in [\"Relu\",\"leaky\"]:\n",
    "                w = np.random.normal(loc=0,scale = np.sqrt(2/(M)),size=(M,N))\n",
    "                w = np.append(w,[[0]*N], axis=0) # BIAS\n",
    "                weights.append(w)\n",
    "            else:\n",
    "                w = np.random.normal(loc=0,scale = np.sqrt(2/(M+N)),size=(M,N)) \n",
    "                w = np.append(w,[[0]*N], axis=0) #BIAS\n",
    "                weights.append(w)\n",
    "        return weights\n",
    "    \n",
    "    def train(self,X,y,lr,batch,max_epoch,eps):\n",
    "        Xsamp = X.sample(batch)\n",
    "        ysamp = y.loc[Xsamp.index]\n",
    "        Losses = []\n",
    "        \n",
    "        # Set up\n",
    "        self.setup(Xsamp.shape[1])\n",
    "        \n",
    "        # first iteration\n",
    "        epoch = 1\n",
    "        weights = self.initialize_weights()\n",
    "        Xsamp[\"ones\"] = 1\n",
    "        intermediates = self.forward_pass(Xsamp,ysamp,weights)\n",
    "        weights = self.back_propagation(Xsamp,ysamp,intermediates,weights,lr,batch)\n",
    "        L0 = intermediates[-1]\n",
    "        Losses.append(L0)\n",
    "        print(\"epoch:\",epoch,\"    Loss:\",L0)\n",
    "        while(L0 > eps):\n",
    "            Xsamp = X.sample(batch)\n",
    "            ysamp = y.loc[Xsamp.index]\n",
    "            Xsamp[\"ones\"] = 1\n",
    "            intermediates = self.forward_pass(Xsamp,ysamp,weights)\n",
    "            weights = self.back_propagation(Xsamp,ysamp,intermediates,weights,lr,batch)\n",
    "            L0 = intermediates[-1]\n",
    "            Losses.append(L0)\n",
    "            epoch = epoch + 1\n",
    "            if epoch == max_epoch:\n",
    "                break\n",
    "        self.weights = weights\n",
    "        print(\"epoch:\",epoch)\n",
    "        #return L0\n",
    "        return Losses\n",
    "    \n",
    "    def predict(self,X_test,y_test):\n",
    "        X_test[\"ones\"] = 1\n",
    "        intermediates = self.forward_pass(X_test,y_test,self.weights)\n",
    "        predictions = intermediates[-2]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data/sberbank-russian-housing-market/train.csv')\n",
    "df_raw = df_raw.select_dtypes(exclude=['category', 'object'])\n",
    "df_raw = df_raw.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_raw.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = set()\n",
    "for col1 in df_raw.columns:\n",
    "    if col1 in remove_cols or col1 == 'price_doc':\n",
    "        continue\n",
    "        \n",
    "    for col2 in df_raw.columns:\n",
    "        if col1 == col2 or col2 in remove_cols or col2 == 'price_doc':\n",
    "            continue\n",
    "            \n",
    "        if abs(df_corr[col1][col2]) > 0.80:\n",
    "            remove_cols.add(col2)\n",
    "            \n",
    "df = df_raw.drop(list(remove_cols), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = df['price_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = 50\n",
    "highest_corrs = abs(df_corr['price_doc']).sort_values(ascending=False)[1:]\n",
    "features = ['price_doc']\n",
    "c = 0\n",
    "for feat in highest_corrs.index:\n",
    "    if feat in set(df.columns):\n",
    "        features.append(feat)\n",
    "        c += 1\n",
    "        \n",
    "    if c >= NUM_FEATURES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('price_doc',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,prices],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price_doc', axis=1)\n",
    "y = np.sqrt(df['price_doc'])\n",
    "\n",
    "X_train = X.iloc[:int(len(X)*0.8)]\n",
    "y_train = y.iloc[:int(len(X)*0.8)]\n",
    "X_test = X.iloc[int(len(X)*0.8)+1:]\n",
    "y_test = y.iloc[int(len(X)*0.8)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw price_doc density plot\n",
    "df['price_doc'].plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqrt(price_doc) density plot\n",
    "y.plot.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "reg = LinearRegression().fit(X_train,y_train)\n",
    "y_pred_lin = reg.predict(X_test)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lin))\n",
    "rmse**2 # back transforming sqrt(price_doc) to price_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(Layers=1, Nodes=[50,30,1], NNodes=None, Activations=[\"Relu\",\"tanh\",\"Relu\"], ActivationFn=None, a=None)\n",
    "start_time = timeit.default_timer()\n",
    "res = NN.train(X_train,y_train,110,4096,100,10**-3)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)\n",
    "print(\"Loss:\", res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x = np.linspace(1, len(res), 7)\n",
    "ax.plot(x, res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN.predict(X_test,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse**2 #backtransorming square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(Layers=1, Nodes=[50,30,1], NNodes=None, Activations=[\"Relu\",\"tanh\",\"Relu\"], ActivationFn=None, a=None)\n",
    "start_time = timeit.default_timer()\n",
    "res = NN.train(X_train,y_train,110,4096,5000,10**-3)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)\n",
    "print(\"Loss:\", res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x = np.linspace(1, len(res), 7)\n",
    "ax.plot(x, res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN.predict(X_test,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse**2 #backtransorming square root"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
